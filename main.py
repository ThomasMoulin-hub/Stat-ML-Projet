#!/user/tmm2219/.conda/envs/statml/bin/python
#import squidpy as sq
import spatialdata as sd
import spatialdata_io as sio
from spatialdata import read_zarr
import pandas as pd
#import spatialdata_plot
import torch
from scipy.sparse import issparse

from model_joint_encoder import create_joint_encoder_model
from data_preprocessing import preprocess_adata
from data_preprocessing_subgraph import build_local_subgraphs, create_subgraph_splits
from train_subgraph import SubgraphTrainer as Trainer


from model import create_model
from evaluate import (evaluate_predictions, plot_training_history,
                      plot_predictions_vs_true, plot_spatial_predictions,
                      plot_error_distribution, analyze_extreme_errors)
import os

import pickle
import json


dataset_name="Xenium_V1_Human_Kidney_FFPE_Protein_updated_outs/"
xenium_path = "./data/" + dataset_name

sdata = sio.xenium(xenium_path, gex_only=False, morphology_focus=False, cells_boundaries=False, nucleus_boundaries=False, cells_labels=False, nucleus_labels=False, cells_as_circles=True)

# R√©cup√®re l'AnnData
adata = sdata.tables["table"]



# # Pipeline GNN pour pr√©diction de coordonn√©es spatiales
#
# Ce notebook impl√©mente un Graph Neural Network (GAT) qui pr√©dit les coordonn√©es
# spatiales des cellules bas√© uniquement sur leurs profils d'expression d'ARN et de prot√©ines.
# Le graphe K-NN est construit sur la similarit√© d'expression (pas les coordonn√©es).
#
# ## ‚öôÔ∏è Deux approches disponibles :
#
# ### Approche 1 : Graphe Global (d√©faut)
# - **Un seul grand graphe** avec toutes les cellules
# - Chaque cellule connect√©e √† ses k voisins les plus proches
# - Le GNN traite tout le graphe en une fois
# - **Plus rapide** mais moins flexible
# - Chaque cellule voit indirectement toutes les autres via les couches GNN
#
# ### Approche 2 : Sous-graphes Locaux (recommand√© pour votre question)
# - **Un sous-graphe par cellule** : 1 cellule centrale + 29 voisins = 30 n≈ìuds
# - Le GNN pr√©dit UNIQUEMENT la position de la cellule centrale
# - Traitement par batches de sous-graphes
# - **Plus conforme √† votre description** : chaque point d'entra√Ænement = 1 sous-graphe
# - Isolement complet : chaque pr√©diction utilise uniquement son voisinage local




print(f"{'='*60}")
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Device utilis√©: {device}")
print("üéØ Approche: SOUS-GRAPHES LOCAUX")
print("   ‚Ä¢ Chaque point = 1 cellule centrale + 29 voisins")
print("   ‚Ä¢ Pr√©diction uniquement de la cellule centrale")
print("   ‚Ä¢ Traitement par batches")
print(f"{'='*60}\n")


# Imports pour le pipeline GNN



# ## 1. Pr√©paration et normalisation des donn√©es


# Pr√©traiter les donn√©es (filtrer et normaliser)
adata_processed = preprocess_adata(adata, normalize_genes=True, normalize_proteins=True)


# Extraire les features et coordonn√©es spatiales
if issparse(adata_processed.X):
    features = adata_processed.X.toarray()
else:
    features = adata_processed.X

spatial_coords = adata_processed.obsm["spatial"]

print(f"Shape des features: {features.shape}")
print(f"Shape des coordonn√©es: {spatial_coords.shape}")


# ## 2. Construction du graphe K-NN bas√© sur similarit√© d'expression



# Approche sous-graphes locaux
print("Construction des sous-graphes locaux...")
# Param√®tres de construction
k_value = 49
metric_value = 'cosine'
cache_dir = 'cache_' + dataset_name
os.makedirs(cache_dir, exist_ok=True)
cache_key = f"subgraphs_k{k_value}_metric_{metric_value}"
subgraphs_path = os.path.join(cache_dir, cache_key + '.pt')
scaler_path = os.path.join(cache_dir, cache_key + '_scaler.pkl')
splits_path = os.path.join(cache_dir, cache_key + '_splits.json')

use_cache = os.path.exists(subgraphs_path) and os.path.exists(scaler_path)
if use_cache:
    print(f"üîÅ Cache d√©tect√©: chargement depuis {cache_dir}/")
    subgraphs_list = torch.load(subgraphs_path, weights_only=False)
    with open(scaler_path, 'rb') as f:
        coords_scaler = pickle.load(f)
else:
    print("üöÄ Pas de cache ou incomplet: construction des sous-graphes")
    subgraphs_list, coords_scaler = build_local_subgraphs(
        features=features,
        spatial_coords=spatial_coords,
        k=k_value,
        metric=metric_value
    )
    # Sauvegarder
    torch.save(subgraphs_list, subgraphs_path)
    with open(scaler_path, 'wb') as f:
        pickle.dump(coords_scaler, f)
    print(f"üíæ Sous-graphes et scaler sauvegard√©s dans {cache_dir}/")

# Cr√©er / charger les splits d'indices
if os.path.exists(splits_path):
    print("üîÅ Chargement des splits depuis le cache")
    with open(splits_path, 'r') as f:
        splits_data = json.load(f)
    train_indices = splits_data['train_indices']
    val_indices = splits_data['val_indices']
    test_indices = splits_data['test_indices']
else:
    print("‚öôÔ∏è Cr√©ation des splits d'entra√Ænement/validation/test")
    train_indices, val_indices, test_indices = create_subgraph_splits(
        n_subgraphs=len(subgraphs_list),
        train_ratio=0.7,
        val_ratio=0.15,
        test_ratio=0.15,
        seed=42
    )
    with open(splits_path, 'w') as f:
        json.dump({
            'train_indices': train_indices,
            'val_indices': val_indices,
            'test_indices': test_indices
        }, f)
    print(f"üíæ Splits sauvegard√©s dans {splits_path}")

# Pour compatibilit√© avec le reste du code
data = subgraphs_list[0]  # Juste pour afficher les infos
print(f"\nExemple de sous-graphe:")
print(f"  ‚Ä¢ N≈ìuds: {data.x.shape[0]} (1 centrale + {data.x.shape[0]-1} voisins)")
print(f"  ‚Ä¢ Features par n≈ìud: {data.x.shape[1]}")
print(f"  ‚Ä¢ Ar√™tes: {data.edge_index.shape[1]}")
print(f"  ‚Ä¢ Cible: position de la cellule centrale uniquement")




# ## 3. Cr√©ation du mod√®le GAT


# Cr√©er le mod√®le avec Joint Encoder
in_channels = subgraphs_list[0].x.shape[1]

# R√©cup√©rer le nombre de g√®nes et prot√©ines
n_genes = (adata_processed.var["feature_types"] == "Gene Expression").sum()
n_proteins = (adata_processed.var["feature_types"] == "Protein Expression").sum()

print(f"\nüìä Modalit√©s biologiques:")
print(f"  ‚Ä¢ G√®nes: {n_genes}")
print(f"  ‚Ä¢ Prot√©ines: {n_proteins}")
print(f"  ‚Ä¢ Total features: {in_channels}")

# Choisir l'architecture: 'joint_encoder' ou 'standard'
use_joint_encoder = True  # Mettre False pour utiliser l'ancien mod√®le

if use_joint_encoder:
    print(f"\nüß¨ Utilisation du Joint Encoder (ARN et Prot√©ines s√©par√©s)")
    model = create_joint_encoder_model(
        n_genes=n_genes,
        n_proteins=n_proteins,
        model_type='large',     # 'base' ou 'large'
        rna_hidden=256,         # Encodeur ARN
        protein_hidden=128,     # Encodeur prot√©ines
        joint_hidden=256,       # Repr√©sentation fusionn√©e
        gat_hidden=256,         # GAT layers
        heads=4,
        dropout=0.4
    )
else:
    print(f"\nüìä Utilisation du mod√®le standard (toutes features concat√©n√©es)")
    model = create_model(
        in_channels=in_channels,
        model_type='large',
        hidden_channels=256,
        heads=4,
        dropout=0.4
    )


# ## 4. Entra√Ænement du mod√®le


# Cr√©er le trainer

trainer = Trainer(
        model=model,
        subgraphs_list=subgraphs_list,
        train_indices=train_indices,
        val_indices=val_indices,
        test_indices=test_indices,
        batch_size=300,
        lr=0.001,
        weight_decay=5e-4,
        device=device
    )



# Entra√Æner le mod√®le
best_model_state = trainer.train(
    epochs=200,
    early_stopping_patience=20,
    verbose=True
)


# Visualiser l'historique d'entra√Ænement
history = trainer.get_history()
plot_training_history(history, save_path='results/training_history.png')


# ## 5. √âvaluation sur l'ensemble de test


# Pr√©dire sur l'ensemble de test

# Pour les sous-graphes, utiliser la m√©thode sp√©cifique
y_pred_test, y_true_test = trainer.predict_all(
    trainer.test_loader,
    denormalize=True,
    coords_scaler=coords_scaler
)



# Calculer les m√©triques
metrics, euclidean_distances = evaluate_predictions(
    y_true_test,
    y_pred_test,
    set_name='Test'
)


# Visualiser pr√©dictions vs r√©alit√©
plot_predictions_vs_true(y_true_test, y_pred_test,
                        save_path='results/predictions_vs_true.png')


# Visualiser les positions spatiales
plot_spatial_predictions(y_true_test, y_pred_test, euclidean_distances,
                        save_path='results/spatial_predictions.png')


# Distribution des erreurs
plot_error_distribution(euclidean_distances,
                       save_path='results/error_distribution.png')


# Analyser les erreurs extr√™mes
worst_cells, best_cells = analyze_extreme_errors(
    y_true_test,
    y_pred_test,
    euclidean_distances,
    top_n=10
)


# ## 6. Sauvegarder le mod√®le
# Cr√©er le dossier results s'il n'existe pas
import os
os.makedirs('results', exist_ok=True)

# Sauvegarder le mod√®le
trainer.save_model('results/spatial_gat_model.pt')


# Sauvegarder les m√©triques dans un fichier CSV
import pandas as pd
metrics_df = pd.DataFrame([metrics])
metrics_df.to_csv('results/test_metrics.csv', index=False)
print("‚úì M√©triques sauvegard√©es dans results/test_metrics.csv")