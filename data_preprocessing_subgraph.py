"""
Version alternative du pr√©processing pour cr√©er des sous-graphes locaux.
Chaque point d'entra√Ænement est un graphe de 30 cellules (1 centrale + 29 voisins).
"""

import numpy as np
import torch
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from torch_geometric.data import Data, Batch
from scipy.sparse import issparse
from tqdm import trange, tqdm
import time


def build_local_subgraphs(features, spatial_coords, k=29, metric='cosine', seed=42):
    """
    Construit des sous-graphes locaux pour chaque cellule.
    Chaque sous-graphe contient la cellule cible + ses k voisins les plus proches.

    Args:
        features: Matrice de features (n_cells, n_features)
        spatial_coords: Coordonn√©es spatiales (n_cells, 2)
        k: Nombre de voisins
        metric: 'cosine' ou 'euclidean'
        seed: Pour reproductibilit√©

    Returns:
        subgraphs_list: Liste de Data objects (un par cellule)
        coords_scaler: Scaler pour d√©normaliser
    """
    print(f"\nConstruction des sous-graphes locaux (k={k}, metric={metric})...")
    print(f"√âtape 1/4: Conversion des features...")
    start_time = time.time()

    # Convertir features en numpy si n√©cessaire
    if issparse(features):
        print(f"  - Conversion de matrice sparse ({features.shape}) vers dense...")
        features_array = features.toarray()
    else:
        features_array = np.array(features)

    n_cells = features_array.shape[0]
    print(f"  ‚úì {n_cells} cellules, {features_array.shape[1]} features ({time.time()-start_time:.2f}s)")

    # K-NN bas√© sur similarit√© d'expression
    print(f"\n√âtape 2/4: Construction de l'index K-NN (n_jobs=-1, metric={metric})...")
    start_time = time.time()
    nbrs = NearestNeighbors(n_neighbors=k+1, metric=metric, n_jobs=-1)
    nbrs.fit(features_array)
    print(f"  ‚úì Index K-NN construit ({time.time()-start_time:.2f}s)")

    # Trouver les voisins par batch pour avoir une progression visible
    print(f"\n√âtape 3/4: Recherche des {k} plus proches voisins pour chaque cellule...")
    start_time = time.time()
    batch_size = 10000  # Traiter par batch de 1000 cellules
    distances_list = []
    indices_list = []

    for batch_start in tqdm(range(0, n_cells, batch_size), desc="  Recherche K-NN"):
        batch_end = min(batch_start + batch_size, n_cells)
        batch_distances, batch_indices = nbrs.kneighbors(features_array[batch_start:batch_end])
        distances_list.append(batch_distances)
        indices_list.append(batch_indices)

    distances = np.vstack(distances_list)
    indices = np.vstack(indices_list)
    print(f"  ‚úì Voisins trouv√©s ({time.time()-start_time:.2f}s)")

    distances = np.vstack(distances_list)
    indices = np.vstack(indices_list)
    print(f"  ‚úì Voisins trouv√©s ({time.time()-start_time:.2f}s)")

    # Normaliser les coordonn√©es
    print(f"\nNormalisation des coordonn√©es spatiales...")
    coords_scaler = StandardScaler()
    spatial_coords_normalized = coords_scaler.fit_transform(spatial_coords)
    print(f"  ‚úì Coordonn√©es normalis√©es")

    # Cr√©er un sous-graphe pour chaque cellule
    print(f"\n√âtape 4/4: Construction des sous-graphes ({n_cells} cellules)...")
    start_time = time.time()
    subgraphs_list = []

    for i in trange(n_cells, desc="  Cr√©ation des sous-graphes"):
        # Cellule centrale = n≈ìud 0
        # Voisins = n≈ìuds 1 √† k
        neighbors = indices[i, 1:k+1]  # Exclure la cellule elle-m√™me

        # Indices des cellules dans ce sous-graphe (centrale + voisins)
        subgraph_cells = np.concatenate([[i], neighbors])

        # Features du sous-graphe (k+1 cellules)
        subgraph_features = features_array[subgraph_cells]

        # Coordonn√©es du sous-graphe
        subgraph_coords = spatial_coords_normalized[subgraph_cells]
        subgraph_coords_original = spatial_coords[subgraph_cells]

        # Cr√©er les ar√™tes: cellule centrale (0) connect√©e √† tous ses voisins (1 √† k)
        edge_list = []
        for j in range(1, k+1):
            # Ar√™te bidirectionnelle
            edge_list.append([0, j])  # centrale -> voisin
            edge_list.append([j, 0])  # voisin -> centrale

        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()

        # Cr√©er l'objet Data
        # MODIFICATION: Inclure les positions de TOUTES les cellules pour supervision compl√®te
        data = Data(
            x=torch.tensor(subgraph_features, dtype=torch.float32),
            edge_index=edge_index,
            y=torch.tensor(subgraph_coords, dtype=torch.float32),  # TOUTES les cellules (k+1)
            y_central=torch.tensor(subgraph_coords[0:1], dtype=torch.float32),  # Garde l'ancienne version pour compatibilit√©
            pos_original=torch.tensor(subgraph_coords_original, dtype=torch.float32),  # Toutes les positions originales
            central_idx=torch.tensor([i], dtype=torch.long)  # Index global de la cellule centrale
        )

        subgraphs_list.append(data)

    print(f"  ‚úì {len(subgraphs_list)} sous-graphes cr√©√©s ({time.time()-start_time:.2f}s)")
    print(f"\n‚úì R√©sum√©:")
    print(f"  - {len(subgraphs_list)} sous-graphes cr√©√©s")
    print(f"  - Chaque sous-graphe: {k+1} n≈ìuds, {edge_index.shape[1]} ar√™tes")
    print(f"  - Chaque cible: positions de TOUTES les cellules du sous-graphe (supervision compl√®te)")

    return subgraphs_list, coords_scaler


def create_subgraph_splits(n_subgraphs, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):
    """
    Cr√©e des indices de split pour les sous-graphes.

    Args:
        n_subgraphs: Nombre total de sous-graphes
        train_ratio, val_ratio, test_ratio: Proportions
        seed: Pour reproductibilit√©

    Returns:
        train_indices, val_indices, test_indices: Listes d'indices
    """
    np.random.seed(seed)
    indices = np.random.permutation(n_subgraphs)

    train_size = int(train_ratio * n_subgraphs)
    val_size = int(val_ratio * n_subgraphs)

    train_indices = indices[:train_size].tolist()
    val_indices = indices[train_size:train_size + val_size].tolist()
    test_indices = indices[train_size + val_size:].tolist()


def create_spatial_splits(spatial_coords, train_val_ratio=0.85, seed=42):
    """
    Cr√©e des splits bas√©s sur la position spatiale (axe X).
    Train/Val: moiti√© gauche (X < m√©diane)
    Test: moiti√© droite (X >= m√©diane)

    Args:
        spatial_coords: Coordonn√©es spatiales (n_cells, 2)
        train_val_ratio: Ratio train/(train+val) pour la moiti√© gauche
        seed: Pour reproductibilit√©

    Returns:
        train_indices, val_indices, test_indices: Listes d'indices
    """
    np.random.seed(seed)

    # Extraire les coordonn√©es X
    x_coords = spatial_coords[:, 0]

    # Calculer la m√©diane sur l'axe X
    x_median = np.median(x_coords)

    print(f"\nüìç Split spatial sur l'axe X:")
    print(f"  - M√©diane X: {x_median:.2f}")
    print(f"  - Range X: [{x_coords.min():.2f}, {x_coords.max():.2f}]")

    # Cellules de gauche (X < m√©diane) -> Train/Val
    left_indices = np.where(x_coords < x_median)[0]
    # Cellules de droite (X >= m√©diane) -> Test
    right_indices = np.where(x_coords >= x_median)[0]

    print(f"  - Cellules gauche (train/val): {len(left_indices)} ({100*len(left_indices)/len(x_coords):.1f}%)")
    print(f"  - Cellules droite (test): {len(right_indices)} ({100*len(right_indices)/len(x_coords):.1f}%)")

    # Shuffle les indices de la moiti√© gauche
    np.random.shuffle(left_indices)

    # Split train/val dans la moiti√© gauche
    train_size = int(train_val_ratio * len(left_indices))

    train_indices = left_indices[:train_size].tolist()
    val_indices = left_indices[train_size:].tolist()
    test_indices = right_indices.tolist()

    print(f"  ‚úì Train: {len(train_indices)} cellules (gauche)")
    print(f"  ‚úì Val: {len(val_indices)} cellules (gauche)")
    print(f"  ‚úì Test: {len(test_indices)} cellules (droite)")

    return train_indices, val_indices, test_indices

